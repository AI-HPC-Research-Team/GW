Nestedspace(activation='elu', apply_unconditional_transform=False, base_transform_type='rq-coupling', basis_dir='data/GW150914_sample_prior_basis/', batch_norm=True, batch_size=2048, bw_dstar=None, cuda=True, data_dir='data/GW150914_sample_prior_basis/', detectors=None, distance_prior=None, distance_prior_fn='uniform_distance', dont_sample_extrinsic_only=False, dropout_probability=0.0, epochs=2000, flow_lr=None, hidden_dims=512, kl_annealing=True, lr=0.0002, lr_anneal_method='cosine', lr_annealing=True, mode='train', model_dir='models/GW150914_sample_uniform_100basis_all_uniform_prior/', model_source='new', model_type='nde', nbins=8, nflows=15, nsample=100000, nsamples_target_event=1000, num_transform_blocks=10, output_freq=10, sampling_from='uniform', save=True, save_aux_filename='waveforms_supplementary.hdf5', save_model_name='model.pt', snr_annealing=False, snr_threshold=None, steplr_gamma=0.5, steplr_step_size=80, tail_bound=1.0, truncate_basis=100)
Waveform directory data/GW150914_sample_prior_basis/
Model directory models/GW150914_sample_uniform_100basis_all_uniform_prior/
Device cuda
Loading dataset
Sampling 100000 sets of parameters from uniform prior.
init training...
init relative whitening...
Truncating reduced basis from 600 to 100 elements.
initialidze reduced basis aux...
Building time translation matrices.
calculate threshold standardizatison...
  Generating 90000 detector waveforms
Setting extrinsic parameters to fiducial values.
  Calculating new standardization factors.
sample_extrinsic_only: False
Detectors: ['H1', 'L1']

Constructing model of type nde

Initial learning rate 0.0002
Using cosine LR annealing.

Model hyperparameters:
input_dim 	 15
num_flow_steps 	 15
context_dim 	 400
base_transform_kwargs
	 hidden_dim 	 512
	 num_transform_blocks 	 10
	 activation 	 elu
	 dropout_probability 	 0.0
	 batch_norm 	 True
	 num_bins 	 8
	 tail_bound 	 1.0
	 apply_unconditional_transform 	 False
	 base_transform_type 	 rq-coupling

Training for 2000 epochs
Starting timer
Learning rate: 0.0002
Re-generating waveforms for uniform prior.
Setting extrinsic parameters to fiducial values.
Train Epoch: 1 [0/90000 (0%)]	Loss: 27.4522	Cost: 12.89s
Train Epoch: 1 [20480/90000 (23%)]	Loss: 22.1806	Cost: 6.99s
Train Epoch: 1 [40960/90000 (45%)]	Loss: 21.6007	Cost: 7.44s
Train Epoch: 1 [61440/90000 (68%)]	Loss: 21.5048	Cost: 6.34s
Train Epoch: 1 [81920/90000 (91%)]	Loss: 21.4760	Cost: 6.08s
Train Epoch: 1 	Average Loss: 22.0053
Re-generating waveforms for uniform prior.
Setting extrinsic parameters to fiducial values.
Test set: Average loss: 21.4034

Learning rate: 0.00019999987662997035
Re-generating waveforms for uniform prior.
Setting extrinsic parameters to fiducial values.
Train Epoch: 2 [0/90000 (0%)]	Loss: 21.3795	Cost: 15.63s
Train Epoch: 2 [20480/90000 (23%)]	Loss: 21.3438	Cost: 15.68s
Train Epoch: 2 [40960/90000 (45%)]	Loss: 21.2788	Cost: 12.90s
Train Epoch: 2 [61440/90000 (68%)]	Loss: 21.3029	Cost: 5.93s
Train Epoch: 2 [81920/90000 (91%)]	Loss: 21.2485	Cost: 5.97s
Train Epoch: 2 	Average Loss: 21.3253
Re-generating waveforms for uniform prior.
Setting extrinsic parameters to fiducial values.
Test set: Average loss: 21.3131

Learning rate: 0.00019999950652018584
Re-generating waveforms for uniform prior.
Setting extrinsic parameters to fiducial values.
Train Epoch: 3 [0/90000 (0%)]	Loss: 21.2767	Cost: 13.67s
Train Epoch: 3 [20480/90000 (23%)]	Loss: 21.2222	Cost: 6.24s
Train Epoch: 3 [40960/90000 (45%)]	Loss: 21.1872	Cost: 11.93s
Train Epoch: 3 [61440/90000 (68%)]	Loss: 21.2079	Cost: 15.22s
Train Epoch: 3 [81920/90000 (91%)]	Loss: 21.2371	Cost: 15.78s
Train Epoch: 3 	Average Loss: 21.2575
Re-generating waveforms for uniform prior.
Setting extrinsic parameters to fiducial values.
Test set: Average loss: 21.2607

Learning rate: 0.00019999888967155963
Re-generating waveforms for uniform prior.
Setting extrinsic parameters to fiducial values.
Train Epoch: 4 [0/90000 (0%)]	Loss: 21.3690	Cost: 13.24s
Train Epoch: 4 [20480/90000 (23%)]	Loss: 21.2478	Cost: 6.63s
Train Epoch: 4 [40960/90000 (45%)]	Loss: 21.2145	Cost: 6.24s
Train Epoch: 4 [61440/90000 (68%)]	Loss: 21.2076	Cost: 8.05s
Train Epoch: 4 [81920/90000 (91%)]	Loss: 21.2164	Cost: 5.97s
Train Epoch: 4 	Average Loss: 21.2158
Re-generating waveforms for uniform prior.
Setting extrinsic parameters to fiducial values.
Test set: Average loss: 21.2096

Learning rate: 0.0001999980260856137
Re-generating waveforms for uniform prior.
Setting extrinsic parameters to fiducial values.
Train Epoch: 5 [0/90000 (0%)]	Loss: 21.2077	Cost: 15.02s
Train Epoch: 5 [20480/90000 (23%)]	Loss: 21.1792	Cost: 11.29s
Train Epoch: 5 [40960/90000 (45%)]	Loss: 21.1458	Cost: 15.86s
Train Epoch: 5 [61440/90000 (68%)]	Loss: 21.2166	Cost: 15.95s
Train Epoch: 5 [81920/90000 (91%)]	Loss: 21.1527	Cost: 15.76s
Train Epoch: 5 	Average Loss: 21.1652
Re-generating waveforms for uniform prior.
Setting extrinsic parameters to fiducial values.
Test set: Average loss: 21.1784

Learning rate: 0.00019999691576447898
Re-generating waveforms for uniform prior.
Setting extrinsic parameters to fiducial values.
Train Epoch: 6 [0/90000 (0%)]	Loss: 21.1631	Cost: 14.11s
Train Epoch: 6 [20480/90000 (23%)]	Loss: 21.1117	Cost: 8.13s
Train Epoch: 6 [40960/90000 (45%)]	Loss: 21.1554	Cost: 6.60s
Train Epoch: 6 [61440/90000 (68%)]	Loss: 21.0825	Cost: 6.33s
Train Epoch: 6 [81920/90000 (91%)]	Loss: 21.1187	Cost: 7.71s
Train Epoch: 6 	Average Loss: 21.1193

Stopping timer.
Training time (including validation): 410.77251744270325 seconds
Saving model
